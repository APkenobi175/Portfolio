# CS3430 Class 2 - 1/7/25 Notes

## Review

* Errors contain conditioning and stability
  * Conditioning is a problem property
  * Stability is an algorithm property
* Conditioning tells us how errors in the input affect errors in the output
* Programs can be well-conditioned or ill-conditioned
  * ill-conditioned problems amplify errors
    * Example: doing mathematics on numbers with very small differences
    * How can you condition a problem?


## Differentiation and Integration

* Truncation Error
  * Lets say you are approximating a sum from i =1 to n. There will be a time when the higher order terms will become irrelevant to the sum because of memory limitations. That is truncation error.
* Truncation Error
  * Let's say you are approximating a sum from $i=1$ to $n$. There will be a time when the higher-order terms will become irrelevant to the sum because of memory limitations. That is truncation error.
* Roundoff Error
* Roundoff Error
  * Calculated using $O(\text{machine\ epsilon}/h)$ where $h$ is the step size
  * $F'(x)=\frac{f(x+h)-f(x)}{h}$
  * As $h$ approaches $0$, the truncation error decreases, but the roundoff error increases.
  * There is an optimal $h$ that minimizes the total error.
  * The roundoff error is $O(\varepsilon/h)$; remember $\varepsilon$ denotes the machine epsilon.
* Forward difference
* Forward difference
  * This problem is forward differencing because we are using $f(x+h)$ and $f(x)$.
  * This means we are going forward, beyond $x$.
* CDD (Central Divided Difference)
  * We will be calculating the middle-section derivative instead of the forward difference.
  * You are doing forward and backward differencing, so the error terms will cancel each other.
  * So the derivative of $f(x)$ will not have as much error.
  * Central divided difference formula:
    * $F'(x)=\dfrac{f(x+h)-f(x-h)}{2h}$

* Examples:
  * $F(x+h) = f(x) + h f'(x) + \frac{h^2}{2} f''(x) + \frac{h^3}{6} f'''(x) + O(h^4)$ + ... - $F(x-h) = f(x) - h f'(x) + \frac{h^2}{2} f''(x) - \frac{h^3}{6} f'''(x) + O(h^4)$ + ... = $2 h f'(x) + 2 h^3/6 f'''(x) + ...$ = $f'(x) + O(h^2)$ 
  * This shows us how to calculate the error term for the central divided difference formula.
  * TThe first term $F(x+h)$ represents the Taylor series expansion of $f(x+h)$ around $x$. 
  * The second term $F(x-h)$ represents the Taylor series expansion of $f(x-h)$ around $x$.
  * We subtract them to find the MIDDLE term, which is the derivative. We take upper and lower terms to cancel leaving us with the middle, the derivative, with less error.

* Error will go up to some value of h, then it will start increasing in a dramatic way, its inevitable. How do we get the optimal h?
* IF you know the true error, you can keep comparing and miniimizing the error to find the optimal h.

### Error Analysis of CDD

* See Paper Notes for more examples and details.

```python
import numpy as np
def central_diff(f, h, x):
    # F: the function
    # h: step size
    # x: point to evaluate the derivative
    return (f(x+h) - f(x-h)) / (2*h)

t = np.sin
gt = np.cos(1.0)  # grand truth
hs = [10**(-k) for k in range(1, 13)] # This gives us from 10^-1 to 10^-12
    for h in hs:
    approx = central_diff(t, 1.0, h)
    error = abs(approx - gt) # This finds the absolute error, absolute truth error.
    print(error)
```

* Make sure you read the slides to see the format on how to print out the error
* This is an example of implementing a function to find the central divided difference in Python.
* The error will decrease first, and then you will see that it will stop improving, or you'll see some really wild numbers.  Its beginning to be numerically unstable.
* Using this you can pinpoint the optimal h.

### Homework 1

* You will be given a function, implement this formula and write an error report.
* The homework will be posted on Saturday night, and be due the following Saturday at 11:59 PM. Yippee 7 days is based I will get it done early.

### Richardson Extrapolation

* Lewis Fry Richardson (1881-1953)
* How to do it:
  * $A*$ = The ideal value we want to approximate
  * $A(h)$ (not a function, its the approximation with step size h)
  * $Ch^p$ = Leading order error term (constant C times h, to the power of something p)
  * $A* = A(h) - Ch^p$

  * The formula becomes:
    * $A(h) = A* + Ch^p$ ( remember h is the step size, p is the order of the error term)

1. $A(h) = A* + Ch^p + O$(some big O quantity)
2. $A(h/2) = A* + C(h/2)^p$
3. Multiply 2 by 2^p ( p = power)
4. $2^p A(h/2) = 2^p A* + Ch^p + O$(some big O quantity)
5. Subtract 1 from 4:
   * $2^p A(h/2) - A(h) = (2^p - 1) A* + O$(some big O quantity)
6. Solve for $A*$: $A* = \dfrac{2^p A(h/2) - A(h)}{2^p - 1} + O$(some big O quantity)

#### Example of Richardson Extrapolation

* See Paper Notes for more examples and details.
* Remember $A(1.0)$ is approximately $f'(1.0)$ at $x = 0$. at $x = 0$ the grand truth is 1

1. Apply Central Divided Difference to get $A(h)$ and $A(h/2)$
2. Use Richardson Extrapolation formula to get a better approximation of the derivative.
3. Formula for F'(x):
    * $F'(x) = (4D(h/2) - D(h)) / 3$ where D is the central divided difference function.

* Python Code:

```python
import numpy as np
def richardson_from_central(f, x, h):
    D_h = central_diff(f, h, x)
    D_h2 = central_diff(f, h/2, x)
    return (4 * D_h2 - D_h) / 3  # since p = 2 for central difference
    # This is the 1st richardson extrapolation, and you can keep applying it to get better results.
    # The error goes down to a sweet spot, the extrapolation will help you arrive at that sweet spot faster. You shall never know the truth and the truth will never set you free.
    f = np.exp
    gt = np.exp(1.0)  # grand truth
    hs = [10**(-k) for k in range(1, 13)]
    for h in hs:
        approx = richardson_from_central(f, 1.0, h)
        error = abs(approx - gt)
        print(error)
```

### Sympy

* Symbolic Mathematics Library for Python
* Allows you to do algebraic manipulations
* The homework will show you how to use sympy to do some differentiation and integration.
* You will also lambdify functions to make them faster.
* We will talk more about this on monday

```python
import sympy as sp
```
