# CS3430 Class 5 - 1/21/26 Notes

## Announcements

* We are gonna talk about linear systems today
* Assignment will be graded and returned by saturday
* Assignments will be graded every saturday
* Next assignment will cover what we do today, linear systems
* You'll do some fun stuff with linear algebra like image manipulation

## Linear Systems

* Consider this system


3x1 + 2x2 + x3 & = 39
2x1 + 3x2 + x3 & = 34
x1 + 2x2 + 3x3 & = 26
* Put it into matrix form
* **SEE HANDWRITTEN NOTES**

* How to solve a linear system

1. Elementary Row Operations
    1. Row Swap
        * You can push all zero rows to the bottom
    2. Row scaling
        * Take a real number, and multiply every element in a row by that number
        * Used to get leading 1s
    3. Row addition
        * Take a multiple of one row and add it to another row
        * Used to get zeros below leading 1s

2. Fundamental Theorum of Linear Algebra
    * Row equivilence means using only those 3 operations, both the old and new matrix are equivalent

3. Guassian Elimination (ALso known as guassian redcution)
    * **SEE HANDWRITTEN NOTES FOR ALGORITHM**

        $\begin{bmatrix}2 & 1 & | & 1 \\ 5 & 3 & | & 2\end{bmatrix}$
    * X1 = 1
    * X2 = -1

4. Numpy tools to do this

```python
import numpy as np

A = np.array([[2.0, 1.0],
              [5.0, 3.0]]) # Coefficient matrix

B = np.array([1.0, 2.0]) # Constant terms (answers) 

# Numpy will automatically convert them to floats, so manually setting to float is good practice

# Solve for X in AX = B

X = np.linalg.solve(A, B) # Wow thats fucking easy what the hell. Numpy is powerful wow

print(X) # [ 1. -1.] # The commas turn into periods when printed because numpy arrays are weird like that

Ax = A@X # Matrix multiplication using @ operator

np.matmul(A, X) # Another way to do matrix multiplication
print(Ax) # [1. 2.] # Confirms that AX = B
print(Ax - B) # Ax what we have obtained, B is what we have been given. 

# Prints out [-2.22222222220e^-16, 0.000000000000e+00] which is effectively 0 due to floating point precision

error = np.linalg.norm(Ax - B) # This is how we compute the error between what we have and what we want
# In numpy norm computes the Euclidean norm.
print(f"Error: {error}") # Error: 2.449293598294706e-16

```

* Numpy makes solving linear systems super easy
* Quick check in numpy to see if a system is solvable

* There are 2 quick checks in linear algebra to see if a system is solvable
    1. Absolute value of Determinant of A is NOT zero.
      * Use the fish method lol LOLOL
    2. A is singular
    3. Compute the rank of a matrix
        * The number of independant rows that cannot be expressed as a linear combination of other rows
        * If the rank(a) = n (number of variables) then the system is solvable
        * if the rank(a) < n then don't solve it (nxn matrix only)

```python
import numpy as np

detA = np.linalg.det(A) # Determinant of A
rankA = np.linalg.matrix_rank(A) # Rank of A

if rankA == A.shape[1]: # A.shape[1] is the number of columns in A
    print("System is solvable")
else:
    print("System is not solvable")
```

* Why is it unsolvable if rank(a) < n?
  * Becausee when you apply a transformation of the matrix, you are going to be losing a dimension
  * If you have 3 variables, but only 2 independant equations, you are going to end up with a plane in 3D space, which is infinite solutions
  * If you have 3 variables and 3 independant equations, you will end up with a single point in 3D space, which is a unique solution


* Another thing to think about
  * Lets say the det(a) > 0, what does that mean?
    * The parralellatope is gonna shrink, you are gonna end up with a smaller volume
    * If the determinant is small than 0, it will flip mirror image and flip
    * If its smaller than -1 there will be a flip AND the item will expand

    * IF you drop a dimension you can't go back, you have to stay in the same dimension


```python

import numpy as np
try:
    x = np.linalg.solve(A, B)
    return x
except np.linalg.LinAlgError as e:
    print(e)
    x, residuals, rank, s = np.linalg.lstsq(A, B, rcond=None) # If numpy cannot solve with guassian elimination, we can attempt the method of least squares
    # It returns the least-squares solution to a linear matrix equation
    return x
