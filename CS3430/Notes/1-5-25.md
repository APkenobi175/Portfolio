# CS3430 Class 1 - 1/15/25 Notes

## Announcements

* Professor Kulyukin is pronounced cool-you-kin
* He does not use Windows AT ALL. Linux ONLY

## Error and Stability Introduction

### Pi

* pi = circumfrance / diameter
* c(circumfrance) = pi * d
* We will be approximating pi with algorithms

### Randomness

* We will spend some time determining if a sequence of numbers is random or if there is a pattern.

### Floats, Error, Stability

* In Mathemtics real numbers is a continuum.
* Picture a river of quantity, that river cannot be sorted or counted.
* Natural numbers = {1, 2, 3, 4}
* N is a subset of R
* R, Real numbers are finite, discrete, ALWAYS rounded. 

```python
import numpy as np
pi = np.pi 
```

* In this instance pi will be a finite number, it is ROUNDED.

* Something to always remember:
  * Math != CS, and vice versa.
  * What you learn in Math classes are fairy tales

* Formula to remember:
  * X = +- 1.(binary fraction) * 2^exponent
    * Binary fraction is finite sequence of digits
    * Same with exponenet
    * This will be in base 2

* Discover the implementation of a float in python

```python

import math 
x = 10.5
m, e = math.frexp(x)
# M is mantissa, E is exponent
print(x) # 10.5
print(m) # 0.65625
print(e) # 4
print(m * 2 ** e) # 10.5
```

* So, 10.5 = +0.65625*2^4
* if you see a 2 subscript that means its in binary
* Its always finite, so even though some of the numbers repeat forever, it WILL be finite in CS
* Example: 1/3 = ~0.3334 not 0.333333333333333333333333333333333333
* How long that mantissa is depends on your operating system, hardware, programming language

* Remember: Floats are not equally spaced
* This is computer science not mathematics, remember
* in python 0.1 + 0.2 == 0.3 is false, because of how python stores floats, be careful about this, this is a prime example of how this is Computer Science, not Mathematics
* As far as spacing goes, use `numpy.nextafter` to find the next representable float after a given float
* This can help us estimate the gap between representable floats

```python
import numpy as np
np.nextafter(x,y)
x = 1.0
x1n = np.nextafter(x, np.int)
x2 = 1e10 # 10 billion
x2n = np.nextafter(x2, np.int) # this is the next float after 10 billion
print (x1n - x) # this is estimating the gap between 1 and the next positive float
print (x2n - x2) # THis is estimating the gap between X, 1, and the next positive float, this shows that as numbers get larger, the gap between representable floats gets larger
```

* Machine Epsilon 
  * The smallest number that when added to 1.0 results in a number different from 1.0
  * This is a measure of precision of floating point arithmetic
  * It varies from computer to computer

* Python uses IEEE754 standard
  * This means the Machine Epsilon is approximately 2^-52

```python
import numpy as np
eps = np.finfo(float).eps
print(eps) # This finds the machine epsilon for floats on your machine
```

* Take care of your Machine Epsilon, it is very important when dealing with floats
* 1 + eps > 1. ALAWYS
* Its the smallest possible magnitude that counts
* 1 + eps/2 > 1 is FALSE

### 2 Types of Errors

1. Roundoff Error
    * This is due to the finite representation of numbers in computers
    * When a number cannot be represented exactly, it is rounded to the nearest representable number
    * This can lead to small errors in calculations that can accumulate over time
2. Loss of significance
    * Also known as catastrophic cancellation
    * This can happen when you are subtracting 2 quantities that are close to each other. Some significant digits may be lost

### Examples of Errors

1. Roundoff Error Example:

    ```python
    s = 1e16 # this is a very large number
    for i in range(1000000):
        s += 1e-9 # this is a tiny number
    print(s) # this will print 1e16, because the tiny number is too small
            # to affect the large number due to roundoff error
    ```

2. Loss of Significance Example:

    * G(x) = (1-cos(x)) / (x^2)
    * For small values of x, cos(x) is very close to 1, so the numerator becomes very small
    * This can lead to loss of significance when x is small
    * Limit as x approaches 0 of G(x) is 0.5 exactly, this is MATHEMATICS
    * In python (COMPUTER SCIENCE), the approximation will get worse as x gets smaller due to loss of significance
