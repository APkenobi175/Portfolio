# CS5030 Class 1 - 1/5/26 Notes

## Announcements

* Tests are open book, open notes

### Why High Performance Computing?

* Parrallelism can make a difference in performance
* Years ago performance increases were due to the increasing density of transistors, but there are inherent problems with this approach.
* Moore's Law says that transistors will double every two ears
  * SFaster processors = increased power consumption
  * increased power consumption = increased heat
  * increased heat = unreliable processors
* Why haven't clock speed increased een though transistors have continued to shirnk?
  * Dennard observed that voltage and current should be proportional to the linear dimensions of a transistor
  * Dennard scaling ignored the leakage current, and threshold voltage, which established a baseline of power per transistor
  * As transitors got smaller, power density increases because these don't scale with size
  * As a result clock speeds have plateaued since 2005
* We stopped making things faster, and instead started making more cores
* Core = central processing unit (CPU)
* this is called parrallelism

### Parallelism

* We don't do operations faster, but we do more operations in parallel.

* HPC systems are more than just a collection of CPUS
  * Many important componenets affect performance
    * Network technology and topology
    * Disk I/O
    * Other shared resources

  * Improving preformance of only a single component generates bottlenecks and affects overall performance negatively.
  * Your performance will be bounded by the slowest component

* Parallelism is useless if programmers don't use it
* Serial programs don't benefit from this approach.

* Why do we need ever increasing perfomance?
  * To keep solving larger problems
  * Complex problems are still waiting to be solved
  * Data resolution

### Crisis Management

* Fukushima in March 2011 a tsunami damaged the backup generators of a nuclear power plant
  * Someone had to manage the crisis
  * HPC helped simulating many of these complex problems and provided support for decision making
  * Atmospheric dispursion for example
* Climate modeling
  * You have a lot of data and a lot of models.
  * You can combine and compare these models to get better predictions
* Plane and NASA simulations
  * Simulating airflow over a plane
  * Simulating heat shields on re-entry vehicles
* Covid-19
  * Protein folding simulations
  * Epidemiological modeling
* Big Data
  * Analyzing large datasets
  * Machine learning and AI applications

### More About parallelism

* Why do we need to write programs in parallel?
  * Multiple instances of the same serial program is bad lol and does nothing
  * We need to write programs that can take advantage of multiple cores

### Aproaches to serial problem

* Rewrite serial programs so that they are parallel
* Write translators that convert serial programs to parallel programs
  * This is very difficult and not very effective
* In this course we will focus on the first approach, rewriting serial programs to be parallel

* Sometimes the best approach to parallelism is to step back and devise an entirely new algorithm.

### Example of Master Core Approach

```cpp
sum = 0;
for (i = 0; i < n; i++){
    x = compute_next_value(...)
    sum += x;
}
```

* How can we parallelize this?
* Nesting the for loop?
* We have P cores, P is much smaller than N
* Each core performs a partial sum of approximately N/P values

```cpp
my_sum = 0;
my_first_i =  ...
my_last_i = ...
for (my_i = my_first_i; my_i < my_last_i; my_i++){
    my_x = compute_next_value(...)
    my_sum += x;
}
```

* After each core computes its partial sum, we need to combine these sums into a final result
* My_sum contains the sum of the values computed by its calls to compute_next_value

* Once all the cores are done computing their private my_sums, they form a global sum by sending results to a designated master core, which adds the final resort. 

```cpp
if ("I'm the master core"){
    sum = my_x;
    for each core other than myself{
        receive my_sum from core
        sum += my_sum;
    }
}
```

* Core 0 Will have the answer the others will have a partial sum.
* Can we do better?
* This approach has multiple cores idle while the master core is collecting results
* BEtter parralel algorithm

### Better Parallel Algorithm

* Don't make the master core do all the work
* Share the load with all cores
* We can use a tree structure to combine the results
* This improves utilization of all cores.
* This makes us get our result faster

* If we had 1000 cores, the first example would have 999 cores idle while the master core collected results
* In the tree approach, all cores are busy combining results at each level of the tree. Only 10 recives and 10 additions

### How do we write parallel programs?

* Task Parallelism
  * Example: Grading assignments
    * Devide the assignments amongst the TA
    * OR devide the questions amongst the TAs
  * This is from the first approach

* Data Parallelism
  * Each core works on a different piece of data
  * Example: Image processing
    * Each core processes a different part of the image
  * This is from the second approach

  ### Coordination and Vocabulary

  * Cores usually need to coordinate their work
  * **Communication** - One or more cores send their current partial sums to another core
  * **Load Balancing** - Share the work evenly among the cores so that one is not heavily loaded
  * **Synchronization** - Because each core works at its own pace, make sure cores do not get too far ahead of the rest of the group
  * You can't predict the coordination of the cores
  * That means core 0 could finish before core 1, or vice versa
    * You need to make sure that cores don't get too far ahead of the rest of the group
  * The next step should not start until all the cores are ready.

  * **Sequential Computing** - A programis one in which only one task can be in progress at any instant
  * **Concurrent Computing** - A program is one in which multiple tasks can be in progress at any instant
  * **Parallel computing** - A program is one in which multiple tasks cooperate closely (simultaneously) to solve a problem
  * **Distrubuted computing** - A program may need to cooperate with other programs to solve a problem

* **THE END**
