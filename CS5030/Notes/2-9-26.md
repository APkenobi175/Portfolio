# CS 5030 - Class 10 - 2/9/26 Notes

## Announcements

- Our accounts haven't been made yet because HR is slow. We will get them soon.

## Review from Last Class

- Last class we talked about OpenMP and much easier it is to write parallel code with OpenMP than with pthreads, mutexes, and condition variables.

- it uses an interesting syntax

    ```c
    #pragma omp parallel for num_threads(thread_count)
    ```

- This means that the next block of code will be ran by that many threads in parallel.

- You also need to define reduction variables, and private variables.

  - Reduction - Variables that are shared but each thread has its own copy
  - Private - Variables that are private to each thread ( like factor)
  - Default is public, so if you don't specify anything, it will be shared by all threads.
  - When using openMP, you will need to come up with creative ways to assign threads to work. For example in this code snippet, we are using k%2 to assign even and odd numbers to different threads.

    ```c
    #pragma omp parallel for num_threads(thread_count) reduction(+:sum) private(factor)

    for (k = 0; k<n; k++){
        if (k%2 == 0){
            factor = 1;
        } else {
            factor = -1;
        }
        sum += factor * (1.0 / (2*k + 1));
    }
    ```

  - Rule of thumb - for race conditions, use reduction variables. For thread specific variables, for example, if you have a loop variable that is only used in that thread, use private variables. For example, in the code snippet above, factor is a private variable because it is only used in that thread. use private variables for those.

- Without the reduction you could mark the section as critical and use a barrier and mutexes or busy wait. but this is easier.

- We also talked about scope of variables in openMP. Reduction, private, and shared variables. 
  - Reduction variables each thread has its own copy, this would be the critical section
  - Private variables are private to each thread
  - Shared variables are shared by all threads
- The default clause:
  - if you don't specify anything, it will be shared by all threads. So if you have a variable that is not declared as private or reduction, it will be shared by all threads. This can lead to race conditions
  - if you do `default(none)`, then you have to explicitly declare all variables as private, reduction, or shared. This is a good way to avoid race conditions.

## OpenMP Continued...

### No wait

- A no wait is a way to stop threads from syncronizing at the end of the for loop. 

### Bubble Sort Parallelization

- Non Parallel Bubble Sort:

```c
for (list_length = n; list_length >= 2; list_length--){
    for (i = 0; i<list_length - 1; i++){
        if (a[i] > a[i+1]){
            temp = a[i];
            a[i] = a[i+1];
            a[i+1] = temp;
        }
    }
}
```

### Odd-Even transposition sort

- This is a sorting algorithm that has different phases and based on if the phase is odd or even, it will compare different pairs of elements. This is very parrelizable.

- The phase must be completed before we go to the next phase, so we need a barrier at the end of each phase.

- Parralelize the 2 for loops with openMP.

- So if the phase is even, we take the i-1 index and the i index and compare them, if i-1 is greater than i, we swap them. If the phase is odd, we take the i index and the i+1 index and compare them, if i is greater than i+1, we swap them. We need to do this for n phases, where n is the length of the array.

```c

for (phase = 0; phase < n; phase++){
    if (phase % 2 == 0){
        #pragma omp parallel for num_threads(thread_count) default(none) shared(a, n) private(i, temp)
        for (i = 1; i<n-1; i+=2){ // start at first odd index up to n-1, increment by 2
            // if i - 1 is greater than i, swap them
            if (a[i-1] > a[i]){
                temp = a[i-1];
                a[i-1] = a[i];
                a[i] = temp;
            }
        }
    } else { // if phase is odd
        #pragma omp parallel for num_threads(thread_count) default(none) shared(a, n) private(i, temp)
        for (i = 0; i<n-1; i+=2){ // start at first even index up to n-1, increment by 2
            // if i is greater than i + 1, swap them
            if (a[i] > a[i+1]){
                temp = a[i+1];
                a[i+1] = a[i];
                a[i] = temp;
            }
        }
    }
}
```

- Is the computation safe in this code snippet? Yes, because each thread is only accessing its own private variables and the shared array is only being accessed by one thread at a time
- There is a barrier at the end of the for loop. because we are using openMP, there is an implicit barrier at the end of the for loop, so we don't need to worry about that. We just need to make sure that we are not accessing the same elements of the array at the same time. In this code snippet, we are not accessing the same elements of the array at the same time, so it is safe.

- At the end of each for loop the threads join the main thread again, and the main thread will increment the for loop, and the threads will be created again for the next phase. This is because the initial for loop is created by the master thread.

- If we don't want to keep destroying and creating threads we can put the if else statement inside the for loop, and create threads once. Seperate parralel from for

```c
# pragma omp parallel num_threads(thread_count) default(none) shared(a, n) private(i, temp, phase)
for (phase = 0; phase < n; phase++){
    if (phase % 2 == 0){
        # pragma omp for // this means that the for loop will be parralelized, but the if statement will not be parralelized, so we need to put the if statement outside of the for loop
        for (i = 1; i<n-1; i+=2){ // start at first odd index up to n-1, increment by 2
            // if i - 1 is greater than i, swap them
            if (a[i-1] > a[i]){
                temp = a[i-1];
                a[i-1] = a[i];
                a[i] = temp;
            }
        }
    } else { // if phase is odd
    # pragma omp for
        for (i = 0; i<n-1; i+=2){ // start at first even index up to n-1, increment by 2
            // if i is greater than i + 1, swap them
            if (a[i] > a[i+1]){
                temp = a[i+1];
                a[i+1] = a[i];
                a[i] = temp;
            }
        }
    }
}
```

- `pragma omp for` means that the for loop will be parralelized, but the if statement will not be parralelized, but we don't have to kill and respawn threads for each phase, we just put the if statement inside the for loop, and the threads will be created once, and they will execute the for loop for each phase. This is more efficient than creating and destroying threads for each phase.

- Running the code with two for directives takes much less time, especially if you have a lot of threads.

- Why the change?
  - Parralel for has an implicit barrier at the end of the loop
  - for directive does not, so the threads don't need to die and respawn for each threads they can just be always alive.

### Scheduling Loops

- We want to parallelize loops, but we also want to make sure that the work is evenly distributed among the threads. This is called scheduling.

- We can do an assignment of work using cycling partitioning. This is where we assign the first iteration to the first thread, the second iteration to the second thread, and so on. This is good for load balancing, but it can lead to cache misses because each thread is accessing different parts of the array.

- We want to parallelize this loop

```c
sum = 0.0;
for (i = 0; i<=n; i++){
    sum += f(i);
}
```

```c
double f(int i){
    int j, start = i*(i+1)/2, end = (i+1)*(i+2)/2;
    double sum = 0.0;
    for (j = start; j<end; j++){
        sum += 1.0 / (j + 1);
    }
    return sum;
}
```

- This snippet is not parralelizable because each thread is accessing different parts of the array, so we can use cycling partitioning to assign the iterations to the threads.

- Default scheduliing:

```c
# pragma omp parallel for num_threads(thread_count) default(none) shared(n) reduction(+:sum)
for (i = 0; i<=n; i++){
    sum += f(i);
}
```

- Cycling Scheduling:

```c
# pragma omp parallel for num_threads(thread_count) default(none) shared(n) reduction(+:sum) schedule(static, 1) // static means that the iterations are assigned to the threds in round robin fashion. 1 is the chunk size, which means each thread gets 1 iteration.
for (i = 0; i<=n; i++){
    sum += f(i);
}
```

- We use the schedule key word when we announce our parallel for loop.

- The schedule key word takes 2 arguments.
  - The first is the type of schedule, which can be static, dynamic, or guided.
  - The second is the chunk size, which is the number of iterations that each thread gets. For example, if we have 4 threads and 16 iterations, and we use a chunk size of 2, then each thread will get 2 iterations, and the first thread will get iterations 0 and 4, the second thread will get iterations 1 and 5, the third thread will get iterations 2 and 6, and the fourth thread will get iterations 3 and 7. Then the first thread will get iterations 8 and 12, the second thread will get iterations 9 and 13, the third thread will get iterations 10 and 14, and so on.

- To find the perfect chunk size we can do num of iterations / number of threads, but this is not always the best because some iterations may take longer than others, so we can experiment with different chunk sizes to find the best one for our specific problem.

- **Use static scheduling when the workload is predictable and iterations take the same amount of time**

- Dude openMP is so easy compared to fucking pthreads, mutexes, and condition variables. I love it.

- Dynamic schedule type:
  - This is where we assign the iterations to the threads dynamically, so when a thread finishes its work, it can take on more work. This is good for load balancing, but it can lead to more overhead because the threads need to communicate with each other to get more work.
  - This continues until all iterations are completed. Default chunk size is 1. Chunksize can be omitted if the dynamic schedule type is being used.

  - **Use dynamic scheduling when the workload is unpredictable**

- Guided schedule type:
  - Similar to dunamic each thread executes a chunk and when a thread finishes a chunk, it requests another one
  - However, in a guided schedule as chunks are completed the size of the new chunk is reduced, so the first few chunks are larger than the last few chunks.
  - If no chunksize is specified the size of the chunks decreases down to 1
  - **Use guided scheduling when the workload is unpredictable but you want to reduce overhead**
  - useful in problems with implicit sorting, for example, adaptive mesh refinement, where the workload is unpredictable but we want to reduce overhead.

- The runtime schedule type:
  - The system uses the enviroment variable OMP_SCHEDULE to determine the schedule type and chunk size. This allows us to change the schedule type and chunk size without recompiling the code.
  - Very useful for testing. When we run the program we specify the schedule type in the args.

### Examples

- Message passing

  - When program begins a single thread will get command line arguments and allocate an array of message queues, one for each thread
  - One or more threads may finish allocating their queues before some others, so we need to use a explicit barrier to make sure that all threads have finished allocating their queues before we start sending messages.
  - After all the threads reached the barrier all the threads can proceed

- The Atomic Directive

  - Unlike critical directive, it can only protect critical sections that consist of a single c assignment statement.

  - `# pragma omp atomic`
  - Faster than using a mutex or critical section
  - Further, the statement must have one of the following forms:
    - `x++`
    - `x--`
    - `x += expr`
    - `x -= expr`
    - `x &= expr`
    - `x |= expr`
    - `x ^= expr`
  - Atomic is perfered over critical when you only need to protect a single statement because it has less overhead than a critical section.
  - Many processors provide a special load-modify-store instruction
  - A critical section that only does a load-modify-store can be protected much more efficiently by using this special instruction than by using a mutex or critical section.

- Critical Sections
  - openMP provides the option of adding a name to a critical directive
  - `# pragma omp critical name`
  - When we do this two blocks protected with critical directives with different names can be executed simultaneously
  - However, the names are set during compilation and we want a different critical section for each thread's queue.
  - You can name your critical sections so you can have multiple.
  - This is dangerous because if you have a lot of threads, you can have a lot of critical sections, and it can be hard to keep track of which critical section is for which thread.
- Some Caveats
  - YOu should not mux different types of mutual exclusion for a single critical section
  - There is no guarantee of fairness in mutual exclusion contracts
  - it can be dangerous to nest critical sections.

```c
for (sent_msgs = 0; sent_msgs < send_max; sent_msgs++){
    send_msg();
    try_recieve_msg();
}
while (!Done()){
    try_recieve_msg();
}
```

- Lets parallelize this code snippet. We can use dynamic scheduling because we don't know how long each iteration will take.

```c
mesg = random();
dest = random() % thread_count;
# pragma omp critical
send_msg(dest, mesg);
```

```c
if (queue_size == 0) return;
else if (queue_size == 1){
 #   pragma omp critical
    Dequeue(queue, &src, &mesg);

} else{
    Dequeue(queue, &src, &mesg);
}
Print_message(src, mesg);
```

- These examples use the critical section to ensure that only one thread is accessing the queue at a time. This is necessary because the queue is a shared resource and we need to prevent race conditions.

### Termination 