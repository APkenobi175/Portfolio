# CS5030 - Class 11 - 2/11/26 Notes

## Announcements

- Get your u of u thing to work, its on you not him
- We want access to u of u's data center so we can practice programming GPUs and multicore CPUs


### U of U data center info

- Parallel FS
  - THis means that we have a file system that is distributed across multiple machines, and we can access it in parallel. This is important for big data processing, as it allows us to read and write data quickly.
- NFS
  - This is a network file system that allows us to access files over a network. It is commonly used in distributed systems, and it can be used to access files on a remote server.

- What you care about:
  - You have a home directory, and have a scratch directory. Anything you write or read there is going to be faster than using the home directory because the home directory is on a different machine than the scratch directory. The scratch directory is on the same machine as the data center, so it is much faster to read and write data there.

- Ecah cluster has a set of compute nodes, which contains CPU and GPUs. When you log in you are logging into something called a login node, which is different than a compute node. 
  - Login nodes are where you are going to edit files, compile code, and do project. 
  - The compute nodes are where your programs will be executes
  - YOu can use their VSCode server to edit files on the login node, and then you can use ssh to log in to the compute nodes to run your programs.

- You can upload your code to the home directory, if you dont wanna work through a terminal. 

- A few clusters you can access:
  - If you few the cluster overview you can see the status of each cluster, and you can see how many nodes are available.
  - If you loadi nto a terminal for loadPeak
    - Load into a directory scratch/general/lustre
    - Projects aren't there... write in the home directory for now... it should have a folder for each user.

- from your home directory locate the directory where your code is. Now you are in your login node, so you can edit stuff and shit
- How do you load libraries?
  - You can use the module command to load libraries. 
    - `module list` will show you all the modules installed on the system.
    - `module avail` will show you all the modules that are available to load.
      - Bunch of versions of cuda are available. don't pick the wrong one!
    - `module load emacs` will load the emacs module, which will allow you to use emacs to edit files on the login node.
    - `module load gcc` will load the gcc module, which will allow you to compile code on the login node.
    - `module load cuda` will load the cuda module, which will allow you to compile and run cuda code on the compute nodes.
    - `module load openmpi` will load the openmpi module, which will allow you to compile and run mpi code on the compute nodes.
  - You don't wanna run programs on the login node because everyone is sharing the login node, so you want to run your programs on the compute nodes, which are dedicated to running programs.

#### How to run programs on the compute nodes?

- To run on a cluster, you have to submit a job to the cluster
  - A job is a request that basically says I need this much resources for this much time, and then the cluster will schedule your job to run on the compute nodes when those resources are available.
- You get a budget of resources, and you can use those resources to run your programs.
- It also tells you how many hours you can use a core for. 
- If you have a system with 32 cores and you want to use it for 1 hour, you will use 32 core hours.

- run `~]$ myallocations` to see your allocations, which will show you how many resources you have available and how much time you have left on those resources.

- You can request a job using a batch job, which is basically a script that contains the commands to run your program, and then you submit that script to the cluster using the `sbatch` command. The cluster will then schedule your job to run on the compute nodes when the resources you requested are available.

```bash
#SBATCH --time=DD-HH:MM:SS # how long you want to run your program for
#SBATCH --nodes=<number of nodes>
#SBATCH --ntasks-per-node=<numeber of cpu cores per node>
#SBATCH --gres=gpu:type:count # if you want to use gpus, specify the type and count of gpus you want to use OPTIONAL
#SBATCH -o slurmjob-%j.out-%N # this is the name of the output file, which will contain the output of your program. %j is the job id, and %N is the name of the node that your job ran on.
#SBATCH -e slurmjob-%j.err-%N # this is the name of the error file, which will contain the error output of your program. %j is the job id, and %N is the name of the node that your job ran on.
#SBATCH --account=usucs5030
#SBATCH --partition=notchpeak-freecycle

cd /scratch/general/lustre/<username>/ # change to the directory where your code is located

# run the program

./omp_hello 4 # program is omp_hello argument is 4
```

- You can keep the last 2 lines the same, but you need to change the first 4 lines to specify the resources you want to use for your job.

- Monitor the status of jobs in the queue for the user

    ```bash
    squeue -u <username>
    ```

- The rest of the script is literally just a bash script, so you can put any commands you want to run your program in the script. For example, if you want to compile your code and then run it, you can put the commands to compile and run your code in the script.

#### Requesting a job

- The system to requests job is called slurm. The largest super computers use the same system, so learning how to use slurm is important for running programs on super computers.

  - `sbatch batch_test.sh` will submit the job to the cluster

  - Invalid partition error:
    - run `myallocation | grep kingspeak` to see if you have any allocations on kingspeak, if you don't have any allocations on kingspeak, then you can't run jobs on kingspeak, and you need to request an allocation on kingspeak.
    - Copy and paste the command to request and put it in your script
  - use `sqeue -u $USER` to see the status of your jobs. it shows you whos running and where your job is in the queue. `R` means its running, the time is how long it has been running and the node is where your job is running.
  - Don't judge how others are using the cluster lol.
  - PD means pending, which means your job is waiting in the queue to run

  - to see the output you can run `cat slurmjob-<job id>.out-<node name>` to see the output of your program, and `cat slurmjob-<job id>.err-<node name>` to see the error output of your program.

  - to cancel a job you can run `scancel <job_id>`
    - to get the jobID use `squeue -u $USER` to see the status of your jobs, and then you can copy the job id from there and use it to cancel your job.

#### Interactive Allocation

- If you want to run your program interactively, you can request an interactive allocation, which will give you access to a compute node where you can run your program interactively. This is useful for debugging and testing your code, as it allows you to see the output of your program in real time.

- to request an interactive allocation, you can use the `salloc` command, which will give you access to a compute node where you can run your program interactively. For example, if you want to request an interactive allocation with 4 cores and 1 gpu for 1 hour, you can run the following command:

```bash
salloc -n 1 -N 1 -t 0:15:00 -p notchpeak-gpu -A notchpeak-gpu --gres=gpu
```

- This basically gives you a set amount of time on a compute node with the resources you requested, and then you can run your program interactively on that compute node. When you are done, you can exit the interactive session, and the resources will be released back to the cluster.

- So you can run, compile, edit, etc until you run out of time.

- You can see the output in real time, don't need to wait for the files to be written to see the output, which is useful for debugging and testing your code.

- You can exit the interactive session by running the `exit` command, which will release the resources back to the cluster.

#### Cmake Files

- a CMake file is a file that contains instructions for how to build your program. It is used to specify the source files, the libraries you want to link against, and the compiler flags you want to use.

- This is where you find all your specific dependency information and you can specify how to build your program. It is a more modern way to build programs, and it is more flexible than using a Makefile.

- For example in your cmake file you could say `find_package(OpenMP)` this will do the job of #include <omp.h> and linking against the OpenMP library for you, so you don't have to worry about the details of how to link against the OpenMP library.

- Now you don't have to do it specifically in the compile command, you can just specify it in the cmake file, and then when you run cmake to generate the build files, it will automatically link against the OpenMP library for you.

- `target_compile_options(omp_hello PRIVATE -fopenmp)` this will add the -fopenmp flag to the compile command for the omp_hello target, which will tell the compiler to enable OpenMP support when compiling the omp_hello target.

- This is not a requirement but he encourages you to try it out because it makes life easier

- Try to get this done by friday.

- `module spider` will search for a particular module and show you all the versions of that module that are available. For example, if you want to see all the versions of cuda that are available, you can run `module spider cuda` and it will show you all the versions of cuda that are available to load.

### Distributed Memory

- In a distributed memory system, each processor has its own private memory, and the processors communicate with each other by sending messages over a network. This is in contrast to a shared memory system, where all processors have access to a common memory space.

- Anything that has to be shared has to pass through the interconnect.

- There is a standard called MPI (Message Passing Interface)

#### MPI (Message Passing Interface)

- There are multiple MPI packages, and the one we will use is called OpemMPI, which has nothing to do with openMP lol. 
- OpenMP is integrated into gcc now so you don't have to do anything special to use it tbh.

- Other implementations include MPICH, and MVAPICH, which are also popular MPI implementations.

- You need an `MPI_Init` function call to initialize the MPI environment, and then you can use other MPI functions to send and receive messages between processors.

  - This is where you start using MPI

- `MPI_Finalize` is used to clean up the MPI environment when you are done using it.

- `MPI_Recv` is used to receive a message from another processor, and `MPI_Send` is used to send a message to another processor.

- `MPI_Comm_rank(MPI_COMM_WORLD, &my_rank)` is used to get the rank of the current process. The rank is a unique identifier for each process, and it is used to determine which process is which when sending and receiving messages.

- `MPI_Comm_size(MPI_COMM_WORLD, &num_procs)` is used to get the total number of processes in the MPI environment. This is useful for determining how many processes are available to send and receive messages.

- What is MPI_COMM_WORLD?
  - Its called a communicator, and it is a group of processes that can communicate with each other. MPI_COMM_WORLD is the default communicator that includes all the processes in the MPI environment.
  - Is this like the interconnect? no the communicator uses the interconnect to send messages between processes, but it is not the interconnect itself. The interconnect is the hardware that allows the processes to communicate with each other, while the communicator is a software construct that defines which processes can communicate with each other.



```c

#include <mpi.h>

MPI_Init(NULL, NULL); // initialize the MPI environment

MPI_Send(greeting, strlen(greeting) + 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD); // send a message to another process
// 1. what you want to send (in this case, the greeting message)
// 2. the number of elements you want to send (in this case, strlen(greeting) + 1)
// 3. the data type of the elements you want to send (in this case, MPI_CHAR, which is a character)
// 4. the rank of the destination process (in this case, 0) so basically every rank except 0 will send a message to process 0
// 5. a tag to identify the message (this can be any integer, and it is used to distinguish between different types of messages) 0 is cool
// 6. the communicator that defines which processes can communicate with each other (in this case, MPI_COMM_WORLD, which includes all the processes in the MPI environment)



- 