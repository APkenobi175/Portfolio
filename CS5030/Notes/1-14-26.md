# CS5030 Class 4 - 1/14/25 Notes

## Announcements

- Homework 1 due friday at 11:59 PM

## Review of Last Class

- We talked about instruction level parallelism (ILP)
- We talked about pipelining
- Pipeling is like how we did it in CS2810 with MIPS
- Pipelining has a starting cost, and ending cost
- We talked about multiple issue processors
- We talked about out of order execution
  - Hardware does work while waiting for pipeline stalls

- We talked about smaller simpler cores, and larger complex cores
  - multiple issue processors require much wider chips
  - smaller simpler cores can be replicated more easily

- Flynns Taxonomy (CPU core architectures)
  - SISD - single instruction single data (traditional uniprocessor)
  - SIMD - single instruction multiple data (vector processors, GPUs)
  - MISD - multiple instruction single data (not really used)
  - MIMD - multiple instruction multiple data (modern multicore processors)

- UMA and NUMA architectures
  - UMA - uniform memory access (all processors have equal access time to memory)
  - NUMA - non-uniform memory access (some processors have faster access to some memory)

## Communications

### Distrubuted Memory System

- Clusters (most popular)
  - A collection of commodity systems connected by a high speed network
  - Each system has its own private memory, and then it can communicate with each other through the interconnect
  - An interconnect is a high speed network that connects the different systems in the cluster

  - One processor cannot work on the memory of another processor directly

- what is a node?:
  - A node is a single motherboard with one or more processors, and some amount of memory

### Communication Across Nodes (Interconnections)

- There are switches and they can be organized in different ways
  - Mesh
  - Ring
  - Crossbar
  - Torodial Mesh (by rows and columns wrap around)

- Ideally the best type of connection between nodes:
  - A triangle. Each Node is connected to every other node directly
  - in graph theory this is like a complete graph
  - Some connections are better than others
  - You may need to be careful which processor you are communicating with

- Not too important for this course....

- All we need to know is the difference between local and remote memory access
  - Local memory access is when a processor accesses its own memory
  - Remote memory access is when a processor accesses another processors memory

## Parallel Software

- What can we do in our software to perform parallel operations?

- The burden is on software
  - Hardware and compilers can keep up the pace eeded
  - From now on...
    - In shared memory programs:
      - Start a single process and fork threads
      - Threads carry out tasks
    - In distrubuted memory programs:
      - Start multiple processes
      - Processes carry out tasks

- SPMD - single program multiple data
  - A SPMD program is a single executable that can behave as if it were multiple different programs through the use of conditional brances

  - If im thread process i:
    - Do task i
  - Else if im thread process j:
    - Do task j
  - Else do something else

### Writing Parallel Programs

1. Divide the work among the processes/threads
    1. So each process/thread gets roughly equal amount of work
    2. And communication is minimized
2. Arrange for the processes/threads to synchronize
3. Arrange for the processes/threads to communicate data as needed
4. Remember, you are limited by your SLOWEST process/thread

```cpp
double x[n], y[n];
for (i = 0; i<n; i++){
    x[i] = y[i]
}
```

- This creates 2 arrays of size n
- Each element of x is assigned the value of the corresponding element of y
- 

### Creation of threads

- You can create threads in 2 different ways. Dynamic and static

- **Dynamic** Threads
  - Master thread waits for work, forks new threads, and when threads are done they terminate
  - Efficient use of resources, BUT thread creating and termination is time consuming

- **Static** Threads
  - Pool of threads, created and are allocated work, but do not terminate until cleanup
  - Better performance because threads are not created and destroyed repeatedly
  - BUT bigger potential for wasted resources


- Nondeterminism
  - When threads are running in parallel, the order of execution is not guaranteed

```cpp
printf("Thread %d my_val = %d\n", my_rank, my_x)

```

- In this example thread 1 > my_val = 19
- Thread 0 > my_val = 7

- The threads aren't managed by you, they are managed by the OS
- The OS decides when each thread gets to run
- So the order of execution is not guaranteed
- You will always get the same output, but the order may change

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <pthread.h>
thread_count = 4 // Variable that keeps track of number of threads
thread_handles = malloc(thread_count * sizeof(pthread_t)) // Allocate memory for thread handles

for (thread = 0; thread < thread_count; thread++){
    pthread_create(&thread_handles[thread], NULL, Ciao, (void*) thread);
}

// This basically calls the Ciao function 4 times from 4 different threads

void Ciao(void* rank){
    printf("Ciao from thread %d\n", (int) rank);
    return NULL;
}

// This creates 4 threads that run the Ciao function
// The (void*) thread is passing the thread number to the function
// The NULL is for the thread attributes, we are using default attributes
// The &thread_handles[thread] is the address of the thread handle, and the [thread] is the index of the thread
// So, we are creating threads from 0 to 3, inside the address of thread handles, which we created with malloc

// Now you have to join the threads after they are done

for (thread = 0; thread < thread_count; thread++){
    pthread_join(thread_handles[thread], NULL);
}

```

- Each thread runs the Ciao function
- To make them go in order you could create an array with the amount of elements equal to the number of threads
- If there is no order between threads executing different problems that can cause issues
- Especially if both threads are accessing the same variable
- This is called a race condition, you have to be careful when multiple threads are accessing the same variable

- Example of this:

```c
my_val = computer_val(my_rank);

```

- We can fix this in a few ways
    1. Flag critical section
      - Only ONE thread at a time can access the critical section
    2. Mutually Exclusive
    3. Mutual Exclusion Locks

```c

my_val = Computer=val(my_rank);
Lock($add_my_val_lock);
x += my_val;
Unlock($add_my_val_lock);

```

- Busy-waiting:

```c
my_val = computer_val(my_rank);
if (my_rank == 1){
    while (! ok_for_1);
    x += my_val;
    if (my_rank == 0){
        ok_for_1 = TRUE;
    }
}
```

- What this does is makes thread 1 wait until thread 0 sets the "ok_for_1" variable to TRUE
- This is called busy-waiting because thread 1 is just sitting there waiting for thread 0 to set the variable
- My_rank is the thread number
- The idea is we have a variable that we check periodically to see if the next thread can proceed

- Disadvantages of busy-waiting
  1. You have to wait, its stuck in a while loop, which wastes resources
  2. Works on 2 threads, on many threads it gets complicated unless you go in order
  3. You can use an array, one variable per thread

- We will talk about this more next time

### How we measure performance
(
- Speedup 
  - Number of cores = p
  - Serial runtime = Tserial (time it takes to run on 1 core)
  - Parallel runtime = Tparallel (time it takes to run on p cores)
- linear speedup = Tparallel = Tserial / p 
  - This is ideal.
  - Ratio of time serial vs  time parallel

  - if our speedup is 4 that means the parallel program runs 4 times faster than the serial program

- Speedup of parallel program = Tserial / Tparallel

- Efficiency
  - E = Speedup / p = (Tserial / Tparallel) / p = Tserial / (p * Tparallel)
  - Efficiency is a measure of how well we are using our processors
  - If efficiency is 1, we are using our processors perfectly
  - If efficiency is less than 1, we are not using our processors perfectly
  - If efficiency is greater than 1, we are doing something wrong
  - If speedup is 3, and we have 4 processors, efficiency is 3/4 = 0.75

  
