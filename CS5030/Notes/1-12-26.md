# CS5030 Class 3 - 1/12/26 Notes

## Announcements

* Homework 1.1 due Friday at 11:59 PM
* Homework 1.2 released today, due next Friday at 11:59 PM

## Principle of Locality

* Accessing one location is followed by an access of a nearby location
* **Spatial locality:** accessing data near recently accessed data
* **Temporal locality:** accessing data in the near future

* Its important to know where data is stored so we can access it quickly

### How Cache Works

* Cache is accessed in blocks (cache lines)
* Cache blocks are 8-16 times bigger than memory words
* When reading one entry of an array we are actually fetching a whole 8-16 word block in the cache

```cpp
int A[1024];
for (int i = 0; i < 1024; i++) {
    A[i] = 0; // Accessing A[0] brings in A[
//0] to A[7] into cache
}
```

* Accessing A[0] brings in A[0] to A[7] into cache
* Accessing A[1] to A[7] will be fast because they are in cache
* Accessing A[8] will be slow because it is not in cache

* With cache you can access them in a row or a column. 

### Virtual Memory

* If we run a very large program or a program that accesses very large data setes, all of the instructions and data may not fit in RAM
* Virtual memory allows us to use disk space as an extension of RAM
* Virtual memory functions as a cache for secondary storage (disk)
* it exploits the principle of spatial and temporal locality

### Swap space

* Swap space is a portion of the disk that is used as virtual memory
* Swap space is those parts that are idle are kept in a block of secondary storage called swap space


### Pages

* Pages are blocks of data and instruction
* Usually these are relatively large (4KB to 2MB)

* Virtual page numbers:
  * When a program is compiled its pages are assigned virtual page numbers
  * WHen a program is run, a table is created that maps virtual page numbers to physical page numbers in RAM
  * A page table is used to translate virtual addresses to physical addresses

* Page table:
  * A virtual address table has 2 columns, virtual page number and byte offset
    * The virtual page number is used to look up the physical page number in the page table
    * The byte offset is used to find the specific byte within the physical page

* When data is too large, think about your main memory as cache.
  * When you deal with large datasets you can't store it all in main memory
* You need to develop some strategy to fetch the portions of data that you need This is also called "Out of Core" computing.
* Out of core computing is when you have data that is too large to fit in main memory so you need to fetch portions of data from disk as needed.
* break the problem into tiles, and fetch the tiles as needed.
* work with a budget of memory
* Out of core is when we work on a problem that is larger than the memory is able to hold.

### Example

* Computing pathlines/streamlines in a large vector field
  * Which blocks of data to I fetch first?
  * Which blocks of data do I evict first?

**PROJECT IDEA**



## Instruction Level Paralleism (ILP)

* ILP attempts to improve processor performance by having multiple processor componenets or functional units work on different instructions at the same time
* ILP is a form of parallelism that is implemented within a single processor


### Pipelining

* Example: Add the floating point numbers: 9.87x10^4 and 6.54x10^3

    1. Fetch operands 1 and 2 from memory
    2. Compare exponents
    3. Shift one operand so exponents match
    4. Add
    5. Normalize result
    6. Round result
    7. Store result in memory

* That is 7 steps to add two floating point numbers
* Imagine the processor has 7 units, one for each step

    ```cpp
    float x[1000], y[1000], z[1000];
    for (int i = 0; i < 1000; i++) {
        z[i] = x[i] + y[i];
    }
    ```

* Each iteration of the loop can be broken down into 7 steps
* Each step can be executed in parallel by different functional units
* While one unit is fetching operands for iteration i, another unit can be comparing exponents for iteration i-1, and so on.
* This is called pipelining

* Similar to the laundry example in [CS2810](../../CS2810/Notes/11-21-25.md)

* ONe floating point addition still takes 7 nanoseconds, but if you have multiple floating point additions to do, then you can keep all 7 units busy all the time

### Multiple Issue

* Multiple issue is when the processor can issue multiple instructions per clock cycle
* **Static multiple issue** - functional units are scheduled at compile time
* **Dynamic multiple issue** - functional units are scheduled at run time

* In order to make use of multiple issue, the system must find instructions that can be executed simultaneously

### Speculation

* In speculation, the compiler or processor makes a guess about an instruction and then executes the instruction on the basis of a guess

* How would you make a guess?

  ```cpp
  z = x + y;
  if (z>0){
    w = x;

  } else{
    w = y;
  }
  ```

* OOOH Z will be positive ooooh
* If the system speculates incorrectly it must go back and recalculate the instruction
* Since z is positive MOST of the time we speculate that it will be positive, this saves time, however if its wrong we have to go back and recalculate

* This is specific to the compiler and the architecture

### Improving instruction scheduling

* Pipeliining still leaves some bubbles of compute time that can still be used
* What can we do to fill these bubbles?
* These bubbles will be at the start and end of the pipeline
* We can work on some other instruction that wasn't part of the original loop. Go find instructions that are independent of the current instruction and work on those while we wait for the pipeline to fill up.
* This is called **out of order execution** (ooo)

### OOO Adoption

* Most of the early superscalers were in order designs
* Examples of early ooo designs included MIPS R10000.
* Almost all modern high performance processors are out of order (ooo) designs.

### The debate:

* Can compilers do this job instead of leaving it to hardware?
* Braniac designs are at the smart machine end of the spectrum, with lots of ooo hardware
* Speed-Demon desins are simpler and smaller, relying on a smart compiler to do the scheduling
* OOO hardware should make it possible for more instruction level paralellism to be extracted because things will be known at run time that cannot be predicted in advance by the compiler
* On the other hand, a simpler in order design will be smaller and use less power, which means you can place more small in order cores onto the same chip as fewer large ooo cores
* The debate continues...

* There is NOT a winner yet in this debate


## SMT / Hyperthreading

* Super scaler execution is weakened by the fact that many programs do not have a lot of fine-grain parallelism
* SMT (simultaneous multi threading) or hyperthreading is a technique that allows multiple threads to share the same functional units
* An SMT processor will appear a multi processor system
* SMT requires to duplicate hardware that stores the execution state of each thread, but things like cache and functional units are shared between threads
* The pentium 4 was the first processor to use SMT, which intel called hyperthreading its design allowed for 2 simultaneous threads

### More Cores

* Why build multiple cores if you can just do SMT?
  * Multiple issue processor require a much wider chip area for wiring and placing all the components
  * Several simpler single issues cores can fit the size of a typical smt core 12 to 1
  * This is efficiency cores vs performance cores
  * AMD does not use SMT in its ryzen processors, instead it uses many cores
    * Intel uses a hybrid approach with performance cores and efficiency cores, both with SMT

* Which approach is better?
  * It depends on the application
  * Many applications just don't have many threads, better fewer but faster cores
  * Some applications (database systems, 3d rendering) instead can exploit more threads available, better to have more cores

### Flynn's Taxonomy

* Flynn's taxonomy is a classification of computer architectures based on the number of concurrent instruction and data streams they support

* Its like a grid with 2 rows and 2 columns

  * Single Instruction Single Data (SISD)
    * A single processor executing a single instruction stream to operate on data stored in a single memory
    * Example: traditional uniprocessor systems

  * Single Instruction Multiple Data (SIMD)
    * A single instruction stream is used to operate on multiple data streams simultaneously
    * Example: vector processors, GPUs

  * Multiple Instruction Single Data (MISD)
    * Multiple instruction streams operate on a single data stream
    * Rarely used in practice
    * Example: some fault-tolerant systems
    * Example: Different filters of the same image

  * Multiple Instruction Multiple Data (MIMD)
    * Multiple instruction streams operate on multiple data streams simultaneously
    * Example: modern multi-core processors, distributed systems


## SIMD (Single Instruction Multiple Data)

* What if we don't have as many ALUs as data elements to process?
   * Divide the work and process iteratively
* Ex: m = 4 ALUs,and n = 15 data elements

* Round 1 ALU1 - X[0] , ALU2 - X[1], ALU3 - X[2], ALU4 - X[3]
* Round 2 ALU1 - X[4] , ALU2 - X[5], ALU3 - X[6], ALU4 - X[7]
* Round 3 ALU1 - X[8] , ALU2 - X[9], ALU3 - X[10], ALU4 - X[11]
* Round 4 ALU1 - X[12] , ALU2 - X[13], ALU3 - X[14]

* Total time = 4 rounds
* Divide the calculations among available ALUs.

* **DRAWBACKS**

  * All ALUS are required to perform the same operation, or remain idle
  * in classic design they must also operate synchronously
  * the ALUs have no instruction storage
  * Efficient for large data parallel tasks, but not other types of more complex parallelism


### Vector Processors

* Vector processors operate on arrays or vectors of data while conventional CPUs opeation on individual data elements or scalers
* They are fast, easy to use, vectorizing compilers are good at itendifying code to exploit vector processors
* However, it has HIGH memory bandwidth and can use a lot of your cache
* Very finite limit for scalability


### Graphical Processing Units (GPUs)

* Real time graphics application programming interfaces or APIs use points, lines, and triangles to internally represent the surfaces of 3D objects

* A graphics processing pipeline converts 3D object representations into 2D images

* Shader functions are also implicitly parallel
* GPUS often optimize performance by using SIMD parallelism. 
* The current generation of NVIDIA and AMD GPUs use SIMD architectures to achieve high throughput on graphics workloads

### Vector VS GPU

* Vector processors are designed to handle large vectors of data with a single instruction, while GPUs are designed to handle many small tasks in parallel

    ```cpp
    sum = 0.0;
    for (i = 0; i < N; i++) {
        y[i] += a*x[i];
        sum += z[i]* z[i];
    }
    ```

* How would a vector processor handle this code?
  * It would vectorize the operations, loading large chunks of data into vector registers and performing the operations in parallel
* How would a GPU handle this code?
  * It would launch many threads, each thread handling a small portion of the data, and performing the operations in parallel across the threads
* GPUs are better suited for tasks that can be broken down into many small, independent tasks, while vector processors are better suited for tasks that involve large vectors of data
* So for this case, the GPU would likely be more efficient because it can handle the independent operations on each element of the arrays in parallel across many threads.
* Multiple threads will be reading and writing the same variable sum, which can be a problem. 

* y[i] if m = 8 and p = 2, we are reading and writing y[0] to y[3] in the first iteration. for x[i] we are just reading it. for sum we are reading and writing it.  there is no other thread that will work with y[0] and so for any of those ys because each thread is working on a different value of i. However, for sum, all threads are trying to read and write to the same variable sum.

* Vector processors would need some synchronization mechanism because of the shared variable sum. GPUs can handle this more easily because each thread can have its own copy of sum, and then they can be combined at the end.

* What are the variables that are accessed by multiple threads? This is a good question to ask when designing parallel algorithms.

## MIMD (Multiple Instruction Multiple Data)

* A collection of autonomous processors that are connected to a memory system via an interconnection network
* Each processor can access each memory location
* This is the processor we will use in this course

### UMA and NUMA multicore systems

* UMA (Uniform Memory Access) time to access all the memory locations will be the same for all the cores
* Numa (non uniform memory access) a memory location a core is directy connected to can be accessed faster than a memory location that mustbe accessed through another chip

* Memory is not always accessed the default way

* Distrubuted memory system has each processor with its own local memory, and then connected to other processors via an interconnection network