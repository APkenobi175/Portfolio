# CS5030 Class 5 - 1/21/26 Notes

## Announcements

## Review of Last Class

1. Remember that accessing a 2d array by column is slower than accessing by row due to memory layout. row[a][0] to row[a][8] is in the same memory block, but row[0][a] to row[8][a] is not, and that is why column access is slower. When doing column access, you are getting data from different memory blocks, which means you aren't utilizing cache as well. thats called a cache miss, whereas row access you miss twice and get 8 accesses in a row, which is a cache hit.

2. Also remember that speedup of a parrelel program is how much faster it is than a serial program.

3. A non deterministic parralel program means that the output can vary and be out of order. One of the solutions to this is to use "busy-waiting", which is where a thread will wait until it is safe or its turn to use a shared resource. This will allow the threads to finish in the order that you want them too, and they won't overwrite shared data.

4. If it takes 10 cycles to load a single 64 bit word, then you can use 10 memory banks to load 1 word per cycle. Using memory banks allows you to load multiple sections of a single 64 bit word in parallel, which increases memory bandwidth.

### More on Parallelism

* Busy waiting has some disadvantages, it wastes the CPUs time

### Speedups and efficiencies of a parallel program

* Speedup = Tserial / Tparallel
  * How much faster the parallel program is than the serial program
* Efficiency = Speedup / p
  * p = number of processors
  * Efficiency is a measure of how well we are using our processors
  * If efficiency is 1, we are using our processors perfectly
  * If efficiency is less than 1, we are not using our processors perfectly
  * If efficiency is greater than 1, we are doing something wrong

### Overhead

* Overhead:
  * Overhead is the extra time we are allowed in a budget to execute a parallel program
  * Efficiency decreases as we scale up the number of processors due to overhead
  * However, for larger problems, it is still far more efficient.

* Effect of overhead
  * Tparallel = Tserial / p + Toverhead
  * As p increases, Toverhead becomes more significant
  * This is why efficiency decreases as we increase the number of processors

### Amdahl's Law

* Unless virtually all of the serial program is parallelized, the possible speedup is going to be very limited - Regardless of how many processors you throw at it.

* Example: Can we parallelize this serial program?

```
a = x + y
c = B + a
d = c + a
```

* Each line relies on the previous line, so we cannot parallelize this program.
* This is true for all programs, there is always some serial portion that cannot be parallelized.

* Back to Speedup

  * We can parallelize 90% of a program
  * Parallelization is perfect regardless of number of processors
  * Tserial = 20 seconds
  * Runtime of parallelizable part is:
    * Tparallel_part = 0.9 * Tserial / p
  * Runtime of serial part is:
    * Tserial_part = 0.1 * Tserial = 2
  * Overrall parallel run time is 
    * Tparallel = 0.9 * Tserial / p + 0.1 * Tserial = 18/ p + 2

  * Speedup = Tserial / 0.9 * Tserial / p + 0.1 * Tserial = 20 / (18/p + 2)

### Scalability

* In general a problem is scalable if it can handle ever increasing problem sizes
* IF we increase the number of processes/threads and keep the efficiency fixed without increasing the problem size, this is called **strong scalability**
* IF we keep the efficiency fixed by increasing the problem size at the same rate as we increase the number of processes/threads, this is called **weak scalability**


* Strong scalability is harder to achieve than weak scalability because you are essentially trying to do less work but with more processors, which increases overhead.

* For the project you will do both strong and weak scalability tests and report the results.

* If the time is flat, thats a good scaling. The problem size is changing that is weak scaling. because as you add more processors you are adding more work to keep efficiency the same.
* If time is decreasing, gets small, and then flattens out, that is strong scaling. The problem size is fixed

### Taking Timings

1. Decide what is the runtime of the program
    * Beginning and end.
2. Use functions to calculate the time
    * clock() function in c++ (does not include the time when the program was idle, only when it was running,, that makes it not as useful.)
    * Wall clock time (begining to end of execution, much more useful because it includes idle time)
    * linux command:
      * real: wall clock time
      * user: code running in user mode
      * sys: code running in kermnl mode(e.g., file i/o, etc)
    * These are useful, but in practice its better to program it yourself within the code.

    ```c
    double start, finish;

    start = get_current_time();
    // code to be timed
    finish = get_current_time();
    printf("Elapsed time: %f seconds\n", finish - start);
    ```

* get_current_time() is a function that returns the current time in seconds.
* This is in the programming language called C. not C++ specifically, but C++ can use C functions.

```c
shared double global_elapsed;
private double my_start, my_finish, my_elapsed;
Barrier();
my_start = get_current_time();
// code to be timed
my_finish = get_current_time();
my_elapsed = my_finish - my_start;
global_elapsed = Global_max(global_elapsed, my_elapsed);
Barrier();
if (my_rank == 0){
    printf("Elapsed time: %f seconds\n", global_elapsed);
}
```

* Find the slowest thread time and report that as the elapsed time.
* Barrier() is a function that synchronizes all threads.
* That means that all threads will wait at the barrier until all threads have reached the barrier.
* Global_max() is a function that returns the maximum value of a variable across all threads.

## Paralel Program Design

1. First partion your computation into tasks. 
    * Partitioning: dividing the work into smaller pieces that can be executed in parallel.
2. Communication
    * Determine what data needs to be shared between tasks.

3. Agglomeration or Aggregation
    * Combining tasks to reduce communication overhead.
    * Communication is one of the most expensive operations in a parallel program.

4. Mapping
    * Assigning tasks to processors.
    * Try to minimize communication between processors.
    * Try to balance the load between processors.

* Example: Histogram Calculation (Fosters Method)

  * Serial Algorithm Input
    1. The number of measurments: data_count
    2. An array of data_count floats: data
    3. The minimum value for the bin containing the smallest values: min_meas
    4. THe maximum value for the bin containing the largest values: max_meas
    5. The number of bins: bin_count
  * Serial Algorithm Output
    1. bin_maxes: an array of bin_count floats
    2. bin_counts: an array of bin_count integers

  * What are the tasks in this program?
    1. Find the bin
    2. Increment the bin count

    * Find_bin can be executed independantly for each measurement, completely in parallel.
    * multiple threads can increment the same bin count at the same time, which can lead to race conditions
    * To solve this, we can have local partial solutions for each thread, and then combine them at the end. loc_bin_counts
    * We are trading memory for speed, we are allocating extra arrays for those seperate bin counts.
    * When there is a race condition, we have to serialize that part of the code.


## Shared memory programming with pthreads

* Threads share memory between each other, and are connected via an interconnect. 
* They can access variables in the shared memory space, they can read and write to the same variables.

### POSIX Threads (pthreads)

* Also known as Pthreads
* A standard for unix-like operating systems
* A library that can be linked with C programs
* Its an API for multithreading programming
* Specifies an application programming interface (API) for creating and manipulating threads

* In C++ 11 theres a thread library called <thread> that is similar to pthreads, but pthreads is more widely used and more powerful.

* He will provide examples of both

* Basic pthreads program structure

```c
#include <pthread.h>

int main(int argc, char *argv[]){
    long thread;
    pthread_t *thread_handles; // array of thread handles
    
}



