# CS5030 - Class 9 - 2/4/26 Notes

## Announcements

- Homework 2 due friday

## Review from Last Class

### OpenMP Shared Memory Programming

- OpenMP is an API for shared memory parallel programming
- MP = multiprocessing
- Designed for systems in which each thread or process can potentially have access to all available memory
- System is viewed as a collection of cores or CPUs that share access to a common memory space

#### Pragmas

- Pragmas are special preprocessor instructions
- Typically added to a system to allow behaviors that aren't part of the core C specification
- Compilers that don't support the pragmas ignore them

```c
#include <stdio.h>
#include <omp.h>
#include <stdlib.h>

void Hello(void);

int main(int argc, char*argv[]){
    int thread_count = strtol(argv[1], NULL, 10);
    #pragma omp parallel num_threads(thread_count)
    // This is saying that the following block is to be run in parallel
    Hello();
    return 0;

}
void Hello(void){
    int my_rank = omp_get_thread_num(); // This gets the thread number, its ID
    int thread_count = omp_get_num_threads(); // This gets the total number of threads
    printf("Hello from thread %d of %d\n", my_rank, thread_count);

    // From one line of code we have created multiple threads that run in parallel
}
```

- How to compile a program with OpenMP support:
  - `gcc -g -Wall -fopenmp -o hello hello.c`

#### Terminology

- Pragmas are the most basic way to parallelize code

- Clause
  - Clauses are text that modifies a directive
  - The num_threads clause can be added to a parallel directive
  - It allows the programmer to specify the number of threads to use in a parallel region
- There may be system defined limitations on number of threads a program can start
- The openMP standard doesn't guarantee that this will actually start thread_count threads
- Most current systems can start hundres of even thousands of threads
- Unless we're trying to start a lot of threads, we will almost always get the number of threads we request

- **Team** 
  - A team is a collection of threads that execute a parallel region

- **Parallel Region**
  - A parallel region is a block of code that is executed by multiple threads in parallel

- IN case the compiler doesn't support OpenMP, use an ifdef

```c
#ifdef _OPENMP
#include <omp.h>
#endif
```

- This means that if the _OPENMP is defined in our compiler, we will include the omp.h header file, otherwise we won't. THis ensures that our code will compile even if the compiler doesn't support OpenMP


- Similarly we can use the same idea in our code when we are using OpenMP functions

```c
#ifdef _OPENMP
    int thread_count = omp_get_max_threads();
#else
    int thread_count = 1;
#endif
```

- This way if openMP is not supported our program will just be ran serially instead of in parallel.

#### Trapezoidal Rule with OpenMP


- From calculus remember the trapezoidal rule for approximating integrals

- We can parallelize this using OpenMP

```c
h = (b - a) / n;
approx = (f(a) + f(b)) / 2.0;
for (i = 1; i <= n - 1; i++){
    x_i = a + i * h;
    approx += f(x_i);
}
approx = h * approx;
```

- This is a serial implementation of the trapezoidal rule

- We can construct an OpenMP parallel version of this code

  1. We have identified two types of tasks:
      - computation of the areas of individual trapezoids
      - adding the areas to a global sum
  2. There is no communication among the tasks in the first collection, but each tasks in the first collection must communicate with task 1
  3. We assumed that there would be many more trapazoids than threads
      - So we aggregate tasks by assigning a contiguous block of trapezoids to each thread
      - We could take num of trapezoids / num of threads to get the number of trapezoids per thread

  |thread 0|thread 1|
    |--------|--------|
    |global_result = 0 to register| finish my_result|


- Mutual exclusion. We want to mark our critical section with pragmas.

- Similar to mutex we don't know which thread will go in, but this is just like a mutex lock

```c
void Trap(double a, double b, int n, double* global_result_p);

// Ba bla bla

for (i = 1; i<=local_n; i++){
    x = local_a + i*h;
    my_result += f(x);
}
my_result = my_result * h;

#pragma omp critical
*global_result_p += my_result;
```

- This critical section ensures that only one thread at a time can update the global result
- This is similar to using mutex locks in pthreads
- Why did we learn pthreads if this is better?
  - OpenMP is easier to use for simple parallelism
  - Pthreads gives you more control over the threads and their behavior
  - OpenMP is limited to shared memory systems, pthreads can be used in distributed memory systems as well
  - OpenMP is not as flexible as pthreads for complex parallelism

- Now, remember, this is not ideal because we have a critical section that is doing a global sum. To make this better we can use the tree structured approach we learned in pthreads.

- Every power of 2 thread sends its result to another thread, which sums it up. This way we reduce the number of critical sections and make it more efficient.

- If we do the tree method there is no critical section, because each thread is only writing to its own memory.

```c
global_result = 0.0;
#   pragma omp parallel num_threads(thread_count){
    # pragma omp critical
    global_result += Local_trap(double a, double b, int n);
}
```

- By using local_trap we can avoid the critical section and make it more efficient.

- If we write it like that we force threads to execute sequentially because of the critical section.

- We can avoid this problem by declaring a private variable inside the parallel block and moving the ritical section after the function call

```c
global_result = 0.0;
#   pragma omp parallel num_threads(thread_count){
    double my_result = 0.0;
    my_result = Local_trap(double a, double b, int n);
    # pragma omp critical
    global_result += my_result;
}
```

- This way each thread has its own private variable and we only have a critical section when we update the global result.



#### Reduction Clause

- A reduction operator is a binary operation that combines two values to produce a single value
- A reduction is a computation that repeatedly applies a reduction operator to a set of values to produce a single value
- All of the intermediate results of the operation should be stored in the same variable, which is called the reduction variable

- We can add a reduction clause to a parallel directive to specify that a variable is a reduction variable

```c
reduction(<operator>:<variable>)
```

```c
global_result = 0.0;
#   pragma omp parallel num_threads(thread_count) \
    reduction(+: global_result)
global_result += Local_trap(double a, double b, int n);
}
``` 

- This way we don't need to use a critical section and the compiler will handle the reduction for us.

#### The "Parallel For" Directive

- Parallel for forks a team of threads to execute the following structured block
- However, the structured block following the parallel for directive must be a for loop
- furthermore, with the parallel for directive the system parallelizes the for loop by dividing the iterations of the loop among the threads in the team

```c
h = (b - a) / n;
approx = (f(a) + f(b)) / 2.0;
for (i = 1; i<= n -1; i++){
    x_i = a + i * h;
    approx += f(x_i);
}
approx = h * approx;
```

- This is the serial version of the trapezoidal rule

```c
h = (b - a) / n;
approx = (f(a) + f(b)) / 2.0;
#   pragma omp parallel for num_threads(thread_count) \
    reduction(+: approx) // This tells the compiler to parallelize the for loop and use a reduction on approx
for (i = 1; i<= n -1; i++){
    approx += f(a+ i * h);
}
approx = h * approx;
```

- We use a parallel for directive to parallelize the for loop
- We also use a reduction clause to specify that approx is a reduction variable
- So, we write the code as if we are writing a serial code, but the compiler will handle the parallelization for us and will split the for loop among the threads in the team
- Thats fucking awesome.
- Why didn't we learn this first?
  - Because its important to understand how parallelism works under the hood
  - OpenMP is a high level abstraction that makes it easy to write parallel code, but its important to understand the low level details as well
  - Understanding the low level details will help you write better parallel code and understand the limitations of OpenMP

- Are we going to be using mutexes, semaphores, and conditional variables anymore now that we have OpenMP?
  - Probably not, but its important to understand how they work

- Legal forms for parallelizable for statements

  - For(index = start; (index < end, index<= end, index >= end, index > end); increment)

  - index must be an integer or pointer type
  - during execution of the loop the variable index can only be modified by the increment expression

  - The expressions start, end, and incr must not change during the execution of the loop
  - During execution of the loop the variable index can only be modified by the increment expression


#### Data dependencies

- sometimes we get different results when we run the same parallel code multiple times
- What happens?
  - OpenMP compilers don't check for dependences among iterations in a loop thats being parallelized
    - If there are dependences among iterations, the results may be incorrect
- A loop in which the results of one or more iterations depend on the results of previous iterations is said to have data dependencies

- That means you can't use OpenMP for stuff like fibonacci numbers, because each number depends on the previous two numbers

- to solve this you can ensure that the variable has a private copy for each thread

```c
double factor = 1.0;
double sum = 0.0;
#  pragma omp parallel for num_threads(thread_count) \
    reduction(+: sum) 
for (i = 0; i < n; i++){
    sum += factor/(2*k + 1);
    factor = -factor; // This is a data dependency, because factor depends on the previous value of factor
}
```

- Here, factor is a shared variable, so all threads are accessing the same variable
- To fix this, we can make factor a private variable for each thread

```c
double sum = 0.0;
#  pragma omp parallel for num_threads(thread_count) \
    reduction(+: sum) private(factor)
for (k = 0; k< n; k++){
    if (k % 2 == 0)
        factor = 1.0;
    else
        factor = -1.0;
    sum += factor/(2*k + 1);
}
```

- Now, each thread has its own copy of factor, so there are no data dependencies
- And we determine if the factor is positive or negative based on the value of k and if its even or odd

- You could declare factor inside the for loop as well to make it private, but this uses openMP's private clause to make it explicit

- So rule of thumb, if you have a variable that is being modified inside a parallel for loop, make it private to avoid data dependencies. don't define it before the loop.

- Why don't we need to privatize sum?
  - Because we are using a reduction clause on sum, so each thread has its own copy of sum and the compiler will handle the reduction for us at the end of the loop

- Why do we need to define sum before the loop?
  - Because we need to initialize sum to 0.0 before the loop, otherwise it will have an undefined value
  - The reduction clause will take care of initializing the private copies of sum for each thread to 0.0 

- that is awesome.

#### Scope of Variables

- In serial programming the scope of a variable consists of those parts of a program in which the variable can be referenced

- in openMP the scope of a variable refers to the set of threads that can access the variable in a parallel region

- you define the scope of a variable before the parallel region using clauses in the parallel directive
  - like the reduction clause or the private clause
  - Private scope means that each thread has its own copy of the variable
  - Shared scope means that all threads share the same copy of the variable
  - reduction scope means that each thread has its own copy of the variable and the compiler will handle the reduction for us at the end of the parallel region

- The default scope for variables before the parallel region is shared
  - So if you don't specify a scope for a variable, it will be shared by default

- Theres a way to force variables to be private or shared by default using the default clause
  - if you put default(none) in the parallel directive, then all variables must be explicitly declared as private or shared. every variable must be declared.


- Parralel for loops
  - pragma omp for
  - This directive splits the iterations of the for loop among the threads in the team
  - This directive must be used inside a parallel region

- two for directives vs two parallel for directives
  - two for directives means that the for loops are executed in parallel by the same team of threads
  - two parallel for directives means that each for loop is executed in parallel by a different team of threads
