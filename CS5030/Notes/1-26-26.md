# CS5030 - Class 6 - 1/26/26 Notes

## Announcements

- Issue on homework 1 was mixing up task parralelism and data parallelism.
  - Task parallelism is when different tasks are performed in parallel, while data parallelism is when the same task is performed on different pieces of data in parallel. Different parts of the same problem.
  - How can you do both at the same time?
- Implementations for homework 1:
  - Round robin solution, assign the next tasks to the next available thread
  - Some people sorted the loads and assigned the heaviest tasks first to the least loaded thread. That requires a lot of computation.
  - Something to keep in mind that may take longer to compute the schedule than to just run the tasks.
- Game jam on friday, you don't have homework due on friday, so come to the game jam! (no)

## P Threads

### POSIX Threads (Pthreads)

- Also known as Pthreads
- A standard API for creating and manipulating threads in unix-like operating systems

- How is this happening?
  - You have one main thread that at some point in your program says: I need more threads!
  - One processor allocates a chunk of memory, whenever you create threads, those threads can see that memory.
  - So any variables declared before the thread creation can be seen by all threads.

- Example code:

```c
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

for (thread = 0; thread < thread_count; thread++){ // At this point there is only 1 process, and 1 thread
    pthread_create(&thread_handles[thread], NULL, Hello, (void*) thread); // This creates a new thread. THis is the entry point for the new thread.
}

printf("Hello from the main thread\n");

for (thread = 0; thread < thread_count; thread++){
    pthread_join(thread_handles[thread], NULL); // THis is multiple threads joining back to the main thread. This is where you wait until all threads are done.
} // we will go out of this loop only when we have executed all of those threads

free (thread_handles); // this frees memory allocated for thread handles
return 0;
```

- Syntax:
  - pthread_create(&thread, NULL, function_name, (void*) argument);
    - &thread: pointer to thread handle
    - NULL: thread attributes (NULL means default attributes)
    - function_name: the function that the thread will execute
    - (void*) argument: argument to be passed to the function (cast to void pointer)
  - pthread_join(thread, NULL);
    - thread: thread handle to join
    - NULL: pointer to return value (NULL if not needed)

- Caveats
  - ONly linux and unix based systems support Pthreads natively
  - Windows has its own threading library, but there are ways to use Pthreads on Windows, such as linux subsystems for windows (WSL)

- Compiling a Pthread program
  - gcc -g -Wall -o pth_hello pth_hello.c -lpthread
    - -g: include debugging information
    - -Wall: enable all compiler warnings
    - -o pth_hello: specify output file name
    - pth_hello.c: input source file
    - -l means link to external library
    - -lpthread: link with the Pthreads library

- GDB
  - This is a debugger for C/C++ programs
    - You can use it to step through your code, set breakpoints, and inspect variables
    - To compile with debugging information, use the -g flag
    - Useful to learn now so you can debug your homework assignments
    - If you really wanted to you could just print out a lot of stuff, but that is not efficient

- gcc is the c/c++ compiler for linux systems
  

### Running a Pthread Program

- ./pth_hello <number of threads>
  - This runs the compiled Pthread program with the specified number of threads
  - Example: ./pth_hello 4 would run the program with 4 threads

- Starting the Threads

  - `pthread.h` header file is included to use Pthreads functions
  - `pthread_t` is a data type used to represent a thread
    - You need to create a variable of type `pthread_t` for each thread you want to create
    - ONE OBJECT PER THREAD

```c

int pthread_create(pthread_t* thread_p/* out */,
                     const pthread_attr_t* attr_p/* in */,
                     void* (*start_routine)(void*)/* in */, // This is a void pointer to the function that the thread will execute, pass the name of the function without parentheses
                     void* arg_p/* in */); // The use of void pointer allows passing any type of argument to the thread function. This is just an address in memory. An address to anything.
```

```c
void *Hello(void* rank){
    long my_rank = (long) rank; // because we pass in a void pointer to the function we need to cast it back to the original type
    printf("Hello from thread %ld\n", my_rank);
    return NULL;
}
```

but how do we know rank is a thread id?

- When creating the thread, we pass the thread id as an argument to the thread function
- Each thread will receive its own thread id as an argument
- The thread id is cast to a void pointer when passed to the thread function, and then cast back to the original type inside the function

- A single call to pthread_join waits for the thread associated with pthread_t to terminate

### Threads in C++11

- Easier to program (object oriented)

- Comparison of Pthreads and C++11 threads
  - C++11 threads:
```cpp
  # include <thread>
  # include <iostream>

  void call_from_thread(){
    std::cout << "Hello, World" << std::endl;

  }

  int main(){
    std::thread t1(call_from_thread); // Create a new thread that runs call_from_thread
    t1.join(); // Wait for the thread to finish
    return 0;
  }
```

- std makes sure the compiler uses the C++11 standard
- This is way easier than C and Pthreads lol.

- Defining a thread function

  1. Thread using function pointer

  2. Thread using function object
```cpp
    # include <thread>
    # include <iostream>

    class DisplayTrhead
    {
    public:
        void operator()(){
            std::cout << "Hello from thread object" << std::endl;
        }

    };

    int main(){
        std::thread threadObj((DisplayThread())); // Create a new thread that runs the function object
        for(int i = 0; i < 1000; i++){
            std::cout << "Hello from main thread" << std::endl;
        }
        threadObj.join(); // Wait for the thread to finish, join it back to main thread
        return 0;
    }
```

- WHich is better? Object oriented or function pointer?
  - Function objects are more flexible and can maintain state, while function pointers are simpler and easier to use for simple tasks.
  - Function objects can be reused and can have multiple member functions, while function pointers are limited to a single function.
  - Function objects can be passed as arguments to other functions, while function pointers cannot.

- Lambda Functions

```cpp
    # include <thread>
    # include <iostream>

    int main(){
        std::thread t1([]{
            std::cout << "Hello from lambda thread" << std::endl;
        }); // Create a new thread that runs the lambda function

        t1.join(); // Wait for the thread to finish
        return 0;
    }
```

- Allows you to define functions on the fly without needing to create a separate function or class
- I HATE LAMBDA FUNCTIONS

- Basically the same as function pointers, but using different syntax
- I won't be doing it this way, because I don't like lambda functions
- Good for functions that you won't reuse elsewhere

### Matrix Multiplication Example

- Using the dot product, one at a time you multiply rows of A by columns of B to get elements of C, and then you sum them up. That gives you one row of the output vector C.

- Serial Example:

```c
for (i = 0; i < m; i++){
    y[i] = 0.0;

    for (j = 0; j < n; j++){
        y[i] += A[i][j] * x[j];
    }
}
```

- How do you do this in parallel?
  - For each row of A you can assign a thread to compute that row of C
  - m/p rows per thread, where p is the number of threads
  - As long as you can split m uniformly by p, this will work

- Using 3 Ptrheads:
  - Thread 0: y[0], y[1]
  - Thread 1: y[2], y[3]
  - Thread 2: y[4], y[5]

```c
y[0] = 0.0;
for (j = 0; j < n; j++){
    y[0] += A[0][j] * x[j];
} // Thread 0 computes y[0]

y[i] = 0.0;
for (j = 0; j < n; j++){
    y[i] += A[i][j] * x[j]; // y[i] is computed by thread i / (m/p)
}
```

- Identify the data that we are splitting
  - Figure out what your first row and last row is, and your loop will look like that

```c
long my_rank = (long) rank; // get thread id
int i, j;
int local_m = m / thread_count; // number of rows per thread 
int my_first_row = my_rank * local_m; // first row for this thread - start at my_rank * local_m
int my_last_row = (my_rank + 1) * local_m - 1; // last row for this thread end at (my_rank + 1) * local_m - 1 because we start at 0


for (i = my_first_row; i<= my_last_row; i++){ // from my first row to my last row
    y[i] = 0.0; // initialize y[i] to 0
    for (j = 0; j < n; j++){ // for each column, multiply and accumulate
        y[i] += A[i][j] * x[j]; // compute y[i] i 
    }
}
```

- This is very complex but lets walk through it
  1. define my_rank as the thread id, it is passed into the function
  2. define i and j as loop variables
  3. local_m is the number of rows per thread, so take m (total rows) divided by the thread count, and that gives us how many rows each thread will compute
  4. my_first_row is the first row that THIS thread will compute, so take the rank and multiply it by local_m
  5. my_last_row is the last row that THIS thread will compute, so take (my_rank + 1) (next thread) * (local_m) - 1 (because we start at 0) number of rows per thread minus 1 because we start at 0, how does that give us 0? explain this more

  6. For the homework try to write it yourself so you understand it better

  7. YOu willdo this a lot in this class

- This program is DATA parralele because you are tackling the same problem (matrix multiplication) but splitting the data (rows of A) among multiple threads

- When we split our threads can we do the entire solution with no problems?
  - Yes because each thread has its own row, and its own i and j so it won't conflict with other threads. 

- What if m is not evenly divisible by p?
  - You can assign the remaining rows to the last thread, or distribute them among the threads
  - Example: if m = 10 and p = 3, then each thread gets 3 rows, and the last thread gets 4 rows
  - You can also use a loop to assign rows in a round-robin fashion to threads
    - Example: thread 0 gets rows 0, 3, 6, 9; thread 1 gets rows 1, 4, 7; thread 2 gets rows 2, 5, 8

### Example

- Estimating Pi

```c
double factor = 1.0;
double sum = 0.0;
for (i = 0; i < n; i++, factor = -factor){
    sum += factor / (2 * i + 1);
}
double pi = 4.0 * sum;
```

- Problems with this in parralelism
  - There is a race condition on sum because all the threads will be trying to update it at the same time
  - If we instead of doing factor = -factor, we could call a function that computes the factor based on i, factor = compute_factor(i)
  - When you define a variable think if it needs to be global or private. 
- How do we take care of the sum variable?
  - We can create partial sums for each thread, and then combine them at the end

```c

long my_rank = (long) rank; // get thread id
double factor;
long long i;
long long my_n = n / thread_count; // number of iterations per thread
long long my_first_i = my_rank * my_n; // first iteration for this thread
long long my_last_i = my_first_i _ my_n;

if (my_first_i % 2 == 0){ // Factor is even
    factor = 1.0;
} else {
    factor = -1.0; // Factor is odd
}

for (i = my_first_i; i < my_last_i; i++, factor = -factor){
    sum += factor / (2 * i + 1); // compute partial sum
}
```

- As we increase N the estimate of pi gets better
- We still have a gloval sum variable that is being updated by all threads
- We need to create a local sum variable for each thread, which we can do with busy waiting

```c
for (i = my_first_i; i < my_last_i; i++, factor = -factor){
    while (flag != my_rank); // busy wait
    sum += factor / (2 * i + 1); // compute partial sum
    flag = (flag + 1) % thread_count; // update flag to next thread
}

```

- This is called busy waiting because the thread is constantly checking the flag variable until it is its turn to update the sum variable
- This is inefficient because the thread is wasting CPU cycles while waiting

- We can do better by simply changing `flag = (flag + 1) % thread_count;`

- Critical Section and Partial Sums


### Mutex (Mutual Exclusion)

- A mutex is a special type of variable that can be used to protect a critical section of code from being accessed by multiple threads at the same time

- Used to guarantee that only one thread can access a shared resource at a time
- When you create a mutex, access it, you should always destroy it when you are done with it

```c
pthread_mutex_t mutex; // declare a mutex variable
pthread_mutex_init(&mutex, NULL); // initialize the mutex
pthread_mutex_lock(&mutex); // lock the mutex before entering critical section
// critical section code goes here
pthread_mutex_unlock(&mutex); // unlock the mutex after leaving critical section
pthread_mutex_destroy(&mutex); // destroy the mutex when done
```

- Using Mutex in our PI program

```c

pthread_mutex_lock(&mutex); // lock the mutex before entering critical section
sum += local_sum; // update global sum
pthread_mutex_unlock(&mutex); // unlock the mutex after leaving critical section
```

- Mutex is a shared variable with a lock and unlock mechanism
- There is a performance cost to using mutexes because they require system calls to lock and unlock, which can be slow, but compared to busy-wait when the number of threads increases, the mutex is much better. You need more threads to see the performance benefit of mutexes over busy-waiting.

- Past 16 threads busy waiting gets worse, mutexes get better.

### Summary

- Pthreads is a standard API for creating and manipulating threads in unix-like operating systems
- C++11 provides a built-in threading library that is easier to use than Pthreads
- Data parallelism involves splitting data among multiple threads to perform the same operation
- Mutexes are used to protect critical sections of code from being accessed by multiple threads at the same time, ensuring mutual exclusion.

- This professor is so bad at explaining things lol. Luckily github copilot is here to help me understand this stuff.

